I would like to share my experience of implementing XOR Neural Network in NumPy.I defined a total of 4 numpy arrays, out of which 2 arrays for each layer, one for weights, and others for the biases. My Neural Network structure was 2--->4--->1. Hidden layer weights were the matrix of 2X4, whereas the biases for it were 1X4 and the same way output layer weights had 4X1 biases for this layer was 1X1.In the forward pass, I defined two more variables for storing the output after each layer named as hidden layer output and predicted output. The function which I used was sigmoid for calculating the activation of each layer. At first, the dot product of inputs and hidden weights is done, which produces a 4x4 matrix to which the hidden biases are added, but unlike normal rules of the matrix, addition is not followed as dot product is 4X4. In contrast, biases are 1X4, but here, the same matrix of 1X4 is added to each row, completing operation calculating z=wx+b. Then the sigmoid function is applied to it, which is then stored in hidden layer output in the same way predicted output is found. Now, coming to the backpropagation, it was challenging at first to implement but finally overcome it.My cost function is 0.5*(expected output-predicted output)^2.I defined some variables to calculate each layer's derivatives like d_output,d_hidden_layer and defined another variable for errors for making my code readable. After calculating the derivatives of layers, each layer's weights and biases were updated, and the learning rate was taken 0.1. Another challenge was updating the biases as there was a compatibility issue with the addition of derivative of output to biases directly, for which an in-built function from NumPy called sum was used, to sum up over the rows to match up with biases. Some more variables were declared as loss1; they were meant to store each input case's loss like (0,0). The total number of epochs carried out was 11000 with a learning rate of 0.1.
The predicted output was as follows
(0,0)------>0.03180451
(0,1)------>0.95471354
(1,0)------>0.95871315
(1,1)------>0.05194248
Finally, losses of each input case were plotted against epochs.
One of the challenges I am facing is implementing this in a modular way, like in the form classes and function. I have tried implementing it, but it does not give any satisfactory output.
